Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8804623796002881, spearman: 0.8763303317439618, Total Loss: 0.5264072202621622
Evaluating best combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 6, 9]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [1, 4, 8, 11]; Final Softmax Layer1: [0, 5, 6, 9]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [2, 3, 8, 11];
Final Metrics - pearson: 0.8733586003490463, spearman: 0.8684845042723898, Total Loss: 0.5367617531025664
Evaluating worst combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 8, 11]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [2, 3, 6, 9]; Final Softmax Layer1: [2, 3, 8, 11]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [0, 5, 6, 9];
Final Metrics - pearson: 0.8655639641591116, spearman: 0.8636978113495689, Total Loss: 0.6589156076946157
Evaluating all 1:
Final Gelu Layer1:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8570564477407481, spearman: 0.8528390166208515, Total Loss: 0.6228547517130983
Evaluating all 3:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
Final Metrics - pearson: 0.8782043184059967, spearman: 0.8739159243859494, Total Loss: 0.5420621752421907
Evaluating random1 combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 6, 10]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [2, 3, 8, 11]; Final Softmax Layer1: [0, 5, 8, 9]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [1, 3, 6, 11];
Final Metrics - pearson: 0.8733681359989416, spearman: 0.8685719153646638, Total Loss: 0.5479019807374224
Evaluating random2 combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 8, 11]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [1, 4, 6, 10]; Final Softmax Layer1: [1, 3, 6, 11]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [0, 5, 8, 9];
Final Metrics - pearson: 0.8698426326092169, spearman: 0.8665199406407896, Total Loss: 0.5866664072617571
Evaluating origin:
Final Gelu Layer1:[1, 2, 4, 6]; Final Gelu Layer2: [7, 3, 11, 10]; Final Gelu Layer3: [5, 0, 8, 9]; Final Softmax Layer1: [4, 10, 11, 7]; Final Softmax Layer2: [1, 2, 5, 8]; Final Softmax Layer3: [3, 6, 9, 0];
Final Metrics - pearson: 0.8704157717440545, spearman: 0.8669976544931721, Total Loss: 0.5909985641532756
In the 0'th rl step, updating importance scores...
Current Activation Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Current Softmax Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
RL Action 0, Selected1 Activation Layers: tensor([1, 4, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 11]), Selected3 Activation Layers: tensor([ 0,  3,  7, 10]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.394817978143692
RL Action 1, Selected1 Activation Layers: tensor([ 1,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([2, 3, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([2, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.36879320230335
RL Action 2, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.3659461823105812
RL Action 3, Selected1 Activation Layers: tensor([ 0,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([0, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  7, 10]), Loss: 0.39607979882508515
RL Action 4, Selected1 Activation Layers: tensor([ 0,  4,  7, 11]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([2, 4, 6, 9]), Loss: 0.4002247881889343
RL Action 5, Selected1 Activation Layers: tensor([ 1,  3,  6, 11]), Selected2 Activation Layers: tensor([0, 5, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected3 Softmax Layers: tensor([1, 5, 8, 9]), Loss: 0.383334363847971
RL Action 6, Selected1 Activation Layers: tensor([ 2,  3,  8, 11]), Selected2 Activation Layers: tensor([1, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([1, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  8, 10]), Loss: 0.3618619655817747
RL Action 7, Selected1 Activation Layers: tensor([ 0,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.3756403259560466
RL Action 8, Selected1 Activation Layers: tensor([ 0,  5,  8, 10]), Selected2 Activation Layers: tensor([2, 3, 6, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 11]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  5,  6, 10]), Loss: 0.3938844810426235
RL Action 9, Selected1 Activation Layers: tensor([ 0,  5,  6, 10]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  7, 10]), Selected3 Softmax Layers: tensor([0, 5, 6, 9]), Loss: 0.42063778892159465
RL Action 10, Selected1 Activation Layers: tensor([ 2,  5,  6, 10]), Selected2 Activation Layers: tensor([1, 4, 7, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected2 Softmax Layers: tensor([1, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.38187072061002253
RL Action 11, Selected1 Activation Layers: tensor([ 1,  5,  8, 11]), Selected2 Activation Layers: tensor([0, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.3845040398091078
RL Action 12, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 10]), Selected3 Activation Layers: tensor([ 1,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.3788660614937544
RL Action 13, Selected1 Activation Layers: tensor([0, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3864025292173028
RL Action 14, Selected1 Activation Layers: tensor([ 1,  4,  8, 10]), Selected2 Activation Layers: tensor([0, 5, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected3 Softmax Layers: tensor([2, 5, 8, 9]), Loss: 0.3878549745306373
RL Action 15, Selected1 Activation Layers: tensor([ 2,  3,  7, 10]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected2 Softmax Layers: tensor([1, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.37084407683461906
RL Action 16, Selected1 Activation Layers: tensor([ 1,  4,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([0, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3632724153622985
RL Action 17, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([ 0,  5,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 11]), Loss: 0.37880885116755963
RL Action 18, Selected1 Activation Layers: tensor([ 2,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 8, 9]), Selected3 Activation Layers: tensor([ 0,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  3,  6, 10]), Selected2 Softmax Layers: tensor([0, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  8, 11]), Loss: 0.3654209066927433
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8804623789175081, spearman: 0.8763303317439618, Total Loss: 0.5261341877123142
Evaluating best combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 6, 9]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [1, 4, 8, 11]; Final Softmax Layer1: [0, 5, 6, 9]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [2, 3, 8, 11];
Final Metrics - pearson: 0.8733584732708001, spearman: 0.8684845042723898, Total Loss: 0.5363306292194001
Evaluating worst combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 8, 11]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [2, 3, 6, 9]; Final Softmax Layer1: [2, 3, 8, 11]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [0, 5, 6, 9];
Final Metrics - pearson: 0.8655639803138837, spearman: 0.863696773982305, Total Loss: 0.6564338460089044
Evaluating all 1:
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Final Gelu Layer1:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8570564464354835, spearman: 0.8528390166208515, Total Loss: 0.6217058110744396
Evaluating all 3:
Evaluating origin:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8804623783567074, spearman: 0.8763303317439618, Total Loss: 0.5258928833806769
Evaluating best combination 4-4-4-lr15-group:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
Final Metrics - pearson: 0.8782043234982131, spearman: 0.8739170578386881, Total Loss: 0.5412575775321494
Evaluating random1 combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 6, 9]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [1, 4, 8, 11]; Final Softmax Layer1: [0, 5, 6, 9]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [2, 3, 8, 11];
Final Metrics - pearson: 0.8733585154083463, spearman: 0.8684845042723898, Total Loss: 0.5364989300674581
Evaluating worst combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 6, 10]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [2, 3, 8, 11]; Final Softmax Layer1: [0, 5, 8, 9]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [1, 3, 6, 11];
Final Metrics - pearson: 0.8733685556506258, spearman: 0.8685719153646638, Total Loss: 0.5470583870055827
Evaluating random2 combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 8, 11]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [2, 3, 6, 9]; Final Softmax Layer1: [2, 3, 8, 11]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [0, 5, 6, 9];
Final Metrics - pearson: 0.865563804165716, spearman: 0.8636978113495689, Total Loss: 0.6575312698458103
Evaluating all 1:
Final Gelu Layer1:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8570564510714094, spearman: 0.8528390166208515, Total Loss: 0.6221818327903748
Evaluating all 3:
Final Gelu Layer1:[2, 3, 8, 11]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [1, 4, 6, 10]; Final Softmax Layer1: [1, 3, 6, 11]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [0, 5, 8, 9];
Final Metrics - pearson: 0.8698425192532087, spearman: 0.8665199406407896, Total Loss: 0.5853306307913141
Evaluating origin:
Final Gelu Layer1:[1, 2, 4, 6]; Final Gelu Layer2: [7, 3, 11, 10]; Final Gelu Layer3: [5, 0, 8, 9]; Final Softmax Layer1: [4, 10, 11, 7]; Final Softmax Layer2: [1, 2, 5, 8]; Final Softmax Layer3: [3, 6, 9, 0];
Final Metrics - pearson: 0.8704158728981559, spearman: 0.8669976544931721, Total Loss: 0.5896154074592793
In the 0'th rl step, updating importance scores...
Current Activation Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Current Softmax Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
Final Metrics - pearson: 0.87820429347301, spearman: 0.8739159243859494, Total Loss: 0.5412933970702455
Evaluating random1 combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 6, 10]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [2, 3, 8, 11]; Final Softmax Layer1: [0, 5, 8, 9]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [1, 3, 6, 11];
Final Metrics - pearson: 0.8733680387193367, spearman: 0.8685719153646638, Total Loss: 0.5474400298392519
Evaluating random2 combination 4-4-4-lr15-group:
RL Action 0, Selected1 Activation Layers: tensor([1, 4, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 11]), Selected3 Activation Layers: tensor([ 0,  3,  7, 10]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.3862584035098553
Final Gelu Layer1:[2, 3, 8, 11]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [1, 4, 6, 10]; Final Softmax Layer1: [1, 3, 6, 11]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [0, 5, 8, 9];
Final Metrics - pearson: 0.8698428029479632, spearman: 0.8665199406407896, Total Loss: 0.5859395997955444
Evaluating origin:
Final Gelu Layer1:[1, 2, 4, 6]; Final Gelu Layer2: [7, 3, 11, 10]; Final Gelu Layer3: [5, 0, 8, 9]; Final Softmax Layer1: [4, 10, 11, 7]; Final Softmax Layer2: [1, 2, 5, 8]; Final Softmax Layer3: [3, 6, 9, 0];
Final Metrics - pearson: 0.8704158279891119, spearman: 0.8669976544931721, Total Loss: 0.5900996685662168
In the 0'th rl step, updating importance scores...
Current Activation Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Current Softmax Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
RL Action 1, Selected1 Activation Layers: tensor([ 1,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([2, 3, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([2, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.377117086276412
RL Action 0, Selected1 Activation Layers: tensor([1, 4, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 11]), Selected3 Activation Layers: tensor([ 0,  3,  7, 10]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.3889854256808758
RL Action 2, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.37238776791840794
RL Action 1, Selected1 Activation Layers: tensor([ 1,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([2, 3, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([2, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.36045214258134367
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
RL Action 3, Selected1 Activation Layers: tensor([ 0,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([0, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  7, 10]), Loss: 0.39413122840225695
RL Action 2, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.38472144469618796
RL Action 4, Selected1 Activation Layers: tensor([ 0,  4,  7, 11]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([2, 4, 6, 9]), Loss: 0.3893199313431978
RL Action 3, Selected1 Activation Layers: tensor([ 0,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([0, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  7, 10]), Loss: 0.3759230959042907
RL Action 5, Selected1 Activation Layers: tensor([ 1,  3,  6, 11]), Selected2 Activation Layers: tensor([0, 5, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected3 Softmax Layers: tensor([1, 5, 8, 9]), Loss: 0.3949114379286766
RL Action 4, Selected1 Activation Layers: tensor([ 0,  4,  7, 11]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([2, 4, 6, 9]), Loss: 0.3636968209967017
RL Action 6, Selected1 Activation Layers: tensor([ 2,  3,  8, 11]), Selected2 Activation Layers: tensor([1, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([1, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  8, 10]), Loss: 0.3825510211288929
RL Action 5, Selected1 Activation Layers: tensor([ 1,  3,  6, 11]), Selected2 Activation Layers: tensor([0, 5, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected3 Softmax Layers: tensor([1, 5, 8, 9]), Loss: 0.36895402990281584
RL Action 7, Selected1 Activation Layers: tensor([ 0,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.38969528406858445
RL Action 6, Selected1 Activation Layers: tensor([ 2,  3,  8, 11]), Selected2 Activation Layers: tensor([1, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([1, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  8, 10]), Loss: 0.39798455342650413
RL Action 8, Selected1 Activation Layers: tensor([ 0,  5,  8, 10]), Selected2 Activation Layers: tensor([2, 3, 6, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 11]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  5,  6, 10]), Loss: 0.4103734062984586
RL Action 7, Selected1 Activation Layers: tensor([ 0,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.39369265697896483
RL Action 9, Selected1 Activation Layers: tensor([ 0,  5,  6, 10]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  7, 10]), Selected3 Softmax Layers: tensor([0, 5, 6, 9]), Loss: 0.430236402079463
RL Action 8, Selected1 Activation Layers: tensor([ 0,  5,  8, 10]), Selected2 Activation Layers: tensor([2, 3, 6, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 11]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  5,  6, 10]), Loss: 0.39814793802797793
RL Action 10, Selected1 Activation Layers: tensor([ 2,  5,  6, 10]), Selected2 Activation Layers: tensor([1, 4, 7, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected2 Softmax Layers: tensor([1, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.3794678320735693
RL Action 9, Selected1 Activation Layers: tensor([ 0,  5,  6, 10]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  7, 10]), Selected3 Softmax Layers: tensor([0, 5, 6, 9]), Loss: 0.43256457403302195
RL Action 11, Selected1 Activation Layers: tensor([ 1,  5,  8, 11]), Selected2 Activation Layers: tensor([0, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.3814756239205599
RL Action 10, Selected1 Activation Layers: tensor([ 2,  5,  6, 10]), Selected2 Activation Layers: tensor([1, 4, 7, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected2 Softmax Layers: tensor([1, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.35916019581258296
RL Action 12, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 10]), Selected3 Activation Layers: tensor([ 1,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.33457452364265916
RL Action 11, Selected1 Activation Layers: tensor([ 1,  5,  8, 11]), Selected2 Activation Layers: tensor([0, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.4010794981569052
RL Action 13, Selected1 Activation Layers: tensor([0, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3929641830548644
RL Action 12, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 10]), Selected3 Activation Layers: tensor([ 1,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.3684011285007
RL Action 14, Selected1 Activation Layers: tensor([ 1,  4,  8, 10]), Selected2 Activation Layers: tensor([0, 5, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected3 Softmax Layers: tensor([2, 5, 8, 9]), Loss: 0.39677468098700047
RL Action 13, Selected1 Activation Layers: tensor([0, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3878683781810105
RL Action 15, Selected1 Activation Layers: tensor([ 2,  3,  7, 10]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected2 Softmax Layers: tensor([1, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.37409987930208444
RL Action 14, Selected1 Activation Layers: tensor([ 1,  4,  8, 10]), Selected2 Activation Layers: tensor([0, 5, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected3 Softmax Layers: tensor([2, 5, 8, 9]), Loss: 0.38201981522142886
RL Action 16, Selected1 Activation Layers: tensor([ 1,  4,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([0, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.38469937480986116
RL Action 15, Selected1 Activation Layers: tensor([ 2,  3,  7, 10]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected2 Softmax Layers: tensor([1, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.36729705352336167
RL Action 17, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([ 0,  5,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 11]), Loss: 0.39607940636575223
RL Action 16, Selected1 Activation Layers: tensor([ 1,  4,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([0, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3827002487517893
RL Action 18, Selected1 Activation Layers: tensor([ 2,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 8, 9]), Selected3 Activation Layers: tensor([ 0,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  3,  6, 10]), Selected2 Softmax Layers: tensor([0, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  8, 11]), Loss: 0.36455449819564817
RL Action 17, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([ 0,  5,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 11]), Loss: 0.36632525447756054
RL Action 19, Selected1 Activation Layers: tensor([ 2,  3,  8, 10]), Selected2 Activation Layers: tensor([ 1,  5,  7, 11]), Selected3 Activation Layers: tensor([0, 4, 6, 9]),Selected1 Softmax Layers: tensor([1, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 2,  4,  6, 10]), Loss: 0.36109215818345547
RL Action 18, Selected1 Activation Layers: tensor([ 2,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 8, 9]), Selected3 Activation Layers: tensor([ 0,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  3,  6, 10]), Selected2 Softmax Layers: tensor([0, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  8, 11]), Loss: 0.38453205849975347
RL Action 20, Selected1 Activation Layers: tensor([ 2,  4,  8, 11]), Selected2 Activation Layers: tensor([ 0,  5,  6, 10]), Selected3 Activation Layers: tensor([1, 3, 7, 9]),Selected1 Softmax Layers: tensor([0, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3807918372005224
RL Action 19, Selected1 Activation Layers: tensor([ 2,  3,  8, 10]), Selected2 Activation Layers: tensor([ 1,  5,  7, 11]), Selected3 Activation Layers: tensor([0, 4, 6, 9]),Selected1 Softmax Layers: tensor([1, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 2,  4,  6, 10]), Loss: 0.3541301009431481
RL Action 21, Selected1 Activation Layers: tensor([0, 3, 7, 9]), Selected2 Activation Layers: tensor([ 2,  5,  8, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([2, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 0,  5,  6, 10]), Loss: 0.3807444474473596
RL Action 20, Selected1 Activation Layers: tensor([ 2,  4,  8, 11]), Selected2 Activation Layers: tensor([ 0,  5,  6, 10]), Selected3 Activation Layers: tensor([1, 3, 7, 9]),Selected1 Softmax Layers: tensor([0, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3955883147194982
RL Action 22, Selected1 Activation Layers: tensor([0, 3, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  7, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([1, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 0,  4,  7, 11]), Loss: 0.39916233129799367
RL Action 21, Selected1 Activation Layers: tensor([0, 3, 7, 9]), Selected2 Activation Layers: tensor([ 2,  5,  8, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([2, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 0,  5,  6, 10]), Loss: 0.37190673731267454
RL Action 23, Selected1 Activation Layers: tensor([ 2,  3,  7, 11]), Selected2 Activation Layers: tensor([1, 4, 8, 9]), Selected3 Activation Layers: tensor([ 0,  5,  6, 10]),Selected1 Softmax Layers: tensor([ 1,  3,  7, 11]), Selected2 Softmax Layers: tensor([ 0,  4,  6, 10]), Selected3 Softmax Layers: tensor([2, 5, 8, 9]), Loss: 0.3940159643441439
All action Losses: [0.3862584035098553, 0.377117086276412, 0.37238776791840794, 0.39413122840225695, 0.3893199313431978, 0.3949114379286766, 0.3825510211288929, 0.38969528406858445, 0.4103734062984586, 0.430236402079463, 0.3794678320735693, 0.3814756239205599, 0.33457452364265916, 0.3929641830548644, 0.39677468098700047, 0.37409987930208444, 0.38469937480986116, 0.39607940636575223, 0.36455449819564817, 0.36109215818345547, 0.3807918372005224, 0.3807444474473596, 0.39916233129799367, 0.3940159643441439]
Rewards: [-0.0007499018348199327, 0.005490972120091864, 0.008742189766471009, -0.006079227477076232, -0.0028273188583581854, -0.006605090727852203, 0.0017742924843169305, -0.0030815791679341675, -0.016942309904085295, -0.02998946392206092, 0.0038806400549414866, 0.0025082360165558093, 0.03529771543546578, -0.00529186963166306, -0.007859262778651455, 0.007563405488247188, 0.0003104324391929314, -0.007391538075142812, 0.01416119051960052, 0.016569973989743247, 0.0029753215262182664, 0.0030077046651856287, -0.009463007716135952, -0.006001504412252001]
Probs_Activation:tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000]), Probs_Softmax:tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000])
In the 1'th rl step, updating importance scores...
Current Activation Importance Scores: [0.22889894247055054, 0.11369293183088303, -0.3425918519496918, -0.02634470723569393, -0.02217625267803669, 0.048520952463150024, 0.09277775883674622, -0.07630552351474762, -0.016472216695547104, -0.0541677325963974, 0.05518624186515808, -0.0010184906423091888]
Current Softmax Importance Scores: [-0.05861052870750427, 0.11240427196025848, -0.05379369854927063, 0.13824830949306488, 0.1075311079621315, -0.2457793802022934, -0.08967127650976181, 0.07704557478427887, 0.012625731527805328, -0.08215538412332535, 0.048280514776706696, 0.03387486934661865]
RL Action 22, Selected1 Activation Layers: tensor([0, 3, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  7, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([1, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 0,  4,  7, 11]), Loss: 0.3736325865611434
RL Action 0, Selected1 Activation Layers: tensor([ 0,  3,  6, 10]), Selected2 Activation Layers: tensor([ 1,  4,  8, 11]), Selected3 Activation Layers: tensor([2, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([0, 4, 6, 9]), Loss: 0.3844630786776543
RL Action 23, Selected1 Activation Layers: tensor([ 2,  3,  7, 11]), Selected2 Activation Layers: tensor([1, 4, 8, 9]), Selected3 Activation Layers: tensor([ 0,  5,  6, 10]),Selected1 Softmax Layers: tensor([ 1,  3,  7, 11]), Selected2 Softmax Layers: tensor([ 0,  4,  6, 10]), Selected3 Softmax Layers: tensor([2, 5, 8, 9]), Loss: 0.4091916903480887
All action Losses: [0.3889854256808758, 0.36045214258134367, 0.38472144469618796, 0.3759230959042907, 0.3636968209967017, 0.36895402990281584, 0.39798455342650413, 0.39369265697896483, 0.39814793802797793, 0.43256457403302195, 0.35916019581258296, 0.4010794981569052, 0.3684011285007, 0.3878683781810105, 0.38201981522142886, 0.36729705352336167, 0.3827002487517893, 0.36632525447756054, 0.38453205849975347, 0.3541301009431481, 0.3955883147194982, 0.37190673731267454, 0.3736325865611434, 0.4091916903480887]
Rewards: [-0.004832986744798373, 0.014783813852992767, -0.0019369286064228053, 0.004078003257971585, 0.012524768756013271, 0.008880061727929434, -0.01090473172726858, -0.00801578824003002, -0.011014463690590781, -0.03373418097092906, 0.01568534931919685, -0.012980307137947178, 0.009262475089842015, -0.004075491338763726, -9.560479750725381e-05, 0.01002673971604151, -0.0005598301589021526, 0.010700138651198676, -0.001808012539640913, 0.019206524353687415, -0.009293314418478693, 0.006841402205188141, 0.005652595887322587, -0.018390232446102495]
Probs_Activation:tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000]), Probs_Softmax:tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000])
In the 1'th rl step, updating importance scores...
Current Activation Importance Scores: [0.07158999145030975, -0.015891946852207184, -0.055698033422231674, -0.02437576651573181, 0.02016444504261017, 0.004211321473121643, -0.004306234419345856, -0.16148152947425842, 0.16578777134418488, 0.023573920130729675, -0.13277223706245422, 0.10919833183288574]
Current Softmax Importance Scores: [-0.11141373217105865, 0.07489214837551117, 0.03652159869670868, 0.07124550640583038, 0.10873925685882568, -0.17998477816581726, -0.07448991388082504, 0.11364339292049408, -0.03915349021553993, -0.05977381393313408, -0.05935768410563469, 0.11913152039051056]
RL Action 1, Selected1 Activation Layers: tensor([2, 3, 7, 9]), Selected2 Activation Layers: tensor([ 0,  5,  8, 10]), Selected3 Activation Layers: tensor([ 1,  4,  6, 11]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  7, 11]), Selected3 Softmax Layers: tensor([ 0,  5,  8, 10]), Loss: 0.33954415649175645
RL Action 0, Selected1 Activation Layers: tensor([ 0,  3,  6, 10]), Selected2 Activation Layers: tensor([ 1,  4,  8, 11]), Selected3 Activation Layers: tensor([2, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([0, 4, 6, 9]), Loss: 0.38177882868796587
RL Action 2, Selected1 Activation Layers: tensor([ 1,  4,  7, 10]), Selected2 Activation Layers: tensor([2, 5, 6, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([1, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 11]), Loss: 0.4082217642664909
RL Action 1, Selected1 Activation Layers: tensor([2, 3, 7, 9]), Selected2 Activation Layers: tensor([ 0,  5,  8, 10]), Selected3 Activation Layers: tensor([ 1,  4,  6, 11]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  7, 11]), Selected3 Softmax Layers: tensor([ 0,  5,  8, 10]), Loss: 0.36105658136308194
RL Action 3, Selected1 Activation Layers: tensor([ 1,  3,  7, 11]), Selected2 Activation Layers: tensor([0, 5, 8, 9]), Selected3 Activation Layers: tensor([ 2,  4,  6, 10]),Selected1 Softmax Layers: tensor([0, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 2,  4,  7, 11]), Loss: 0.37712348394095896
RL Action 2, Selected1 Activation Layers: tensor([ 1,  4,  7, 10]), Selected2 Activation Layers: tensor([2, 5, 6, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  3,  6, 11]), Selected2 Softmax Layers: tensor([ 1,  4,  8, 10]), Selected3 Softmax Layers: tensor([2, 5, 7, 9]), Loss: 0.36833742674440145
RL Action 4, Selected1 Activation Layers: tensor([1, 4, 6, 9]), Selected2 Activation Layers: tensor([ 2,  5,  7, 11]), Selected3 Activation Layers: tensor([ 0,  3,  8, 10]),Selected1 Softmax Layers: tensor([ 1,  4,  8, 11]), Selected2 Softmax Layers: tensor([2, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 10]), Loss: 0.3798182413727045
RL Action 3, Selected1 Activation Layers: tensor([ 1,  5,  8, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([2, 3, 7, 9]),Selected1 Softmax Layers: tensor([ 1,  5,  7, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 2,  4,  8, 11]), Loss: 0.4128008569777012
RL Action 5, Selected1 Activation Layers: tensor([ 2,  3,  7, 10]), Selected2 Activation Layers: tensor([ 1,  5,  8, 11]), Selected3 Activation Layers: tensor([0, 4, 6, 9]),Selected1 Softmax Layers: tensor([2, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  7, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  6, 10]), Loss: 0.3555246302485466
RL Action 4, Selected1 Activation Layers: tensor([ 2,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 10]), Selected3 Softmax Layers: tensor([0, 5, 7, 9]), Loss: 0.38512406203895805
RL Action 6, Selected1 Activation Layers: tensor([ 0,  3,  7, 11]), Selected2 Activation Layers: tensor([ 2,  5,  6, 10]), Selected3 Activation Layers: tensor([1, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([ 0,  3,  8, 11]), Selected3 Softmax Layers: tensor([1, 4, 6, 9]), Loss: 0.36382564313709737
RL Action 5, Selected1 Activation Layers: tensor([ 0,  4,  6, 11]), Selected2 Activation Layers: tensor([ 1,  3,  7, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  4,  7, 11]), Selected2 Softmax Layers: tensor([2, 5, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  6, 10]), Loss: 0.40471644192934036
RL Action 7, Selected1 Activation Layers: tensor([0, 4, 7, 9]), Selected2 Activation Layers: tensor([ 2,  5,  8, 10]), Selected3 Activation Layers: tensor([ 1,  3,  6, 11]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.38781238287687303
RL Action 6, Selected1 Activation Layers: tensor([ 0,  4,  7, 11]), Selected2 Activation Layers: tensor([ 2,  3,  8, 10]), Selected3 Activation Layers: tensor([1, 5, 6, 9]),Selected1 Softmax Layers: tensor([0, 4, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  3,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 11]), Loss: 0.36915483240038155
RL Action 8, Selected1 Activation Layers: tensor([2, 3, 8, 9]), Selected2 Activation Layers: tensor([ 0,  4,  7, 11]), Selected3 Activation Layers: tensor([ 1,  5,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 10]), Loss: 0.34295790441334245
RL Action 7, Selected1 Activation Layers: tensor([0, 5, 8, 9]), Selected2 Activation Layers: tensor([ 2,  3,  6, 11]), Selected3 Activation Layers: tensor([ 1,  4,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  3,  7, 11]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([1, 5, 6, 9]), Loss: 0.40625485695898533
RL Action 9, Selected1 Activation Layers: tensor([ 0,  5,  6, 11]), Selected2 Activation Layers: tensor([ 2,  3,  7, 10]), Selected3 Activation Layers: tensor([1, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected2 Softmax Layers: tensor([2, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 11]), Loss: 0.41144802555441856
RL Action 8, Selected1 Activation Layers: tensor([ 2,  3,  8, 10]), Selected2 Activation Layers: tensor([0, 5, 6, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 11]),Selected1 Softmax Layers: tensor([ 2,  4,  6, 10]), Selected2 Softmax Layers: tensor([0, 5, 8, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 11]), Loss: 0.3757512001693249
RL Action 10, Selected1 Activation Layers: tensor([0, 3, 7, 9]), Selected2 Activation Layers: tensor([ 2,  4,  8, 11]), Selected3 Activation Layers: tensor([ 1,  5,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected2 Softmax Layers: tensor([1, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 10]), Loss: 0.3420825655385852
RL Action 9, Selected1 Activation Layers: tensor([2, 4, 8, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 11]), Selected3 Activation Layers: tensor([ 1,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([0, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.3656855836138129
RL Action 11, Selected1 Activation Layers: tensor([ 1,  3,  8, 11]), Selected2 Activation Layers: tensor([0, 5, 6, 9]), Selected3 Activation Layers: tensor([ 2,  4,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  8, 11]), Selected2 Softmax Layers: tensor([0, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  4,  6, 10]), Loss: 0.40587564527988435
RL Action 10, Selected1 Activation Layers: tensor([ 2,  3,  6, 10]), Selected2 Activation Layers: tensor([0, 4, 7, 9]), Selected3 Activation Layers: tensor([ 1,  5,  8, 11]),Selected1 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  6, 10]), Selected3 Softmax Layers: tensor([0, 4, 7, 9]), Loss: 0.36288573473691943
RL Action 12, Selected1 Activation Layers: tensor([2, 3, 6, 9]), Selected2 Activation Layers: tensor([ 1,  4,  7, 10]), Selected3 Activation Layers: tensor([ 0,  5,  8, 11]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([0, 4, 7, 9]), Loss: 0.3644373233988881
RL Action 11, Selected1 Activation Layers: tensor([ 0,  5,  8, 11]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([2, 4, 7, 9]),Selected1 Softmax Layers: tensor([ 1,  4,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  7, 10]), Loss: 0.38280562199652196
RL Action 13, Selected1 Activation Layers: tensor([2, 5, 8, 9]), Selected2 Activation Layers: tensor([ 1,  4,  6, 10]), Selected3 Activation Layers: tensor([ 0,  3,  7, 11]),Selected1 Softmax Layers: tensor([1, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 0,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.40007352448999883
RL Action 12, Selected1 Activation Layers: tensor([ 1,  3,  8, 11]), Selected2 Activation Layers: tensor([ 2,  4,  6, 10]), Selected3 Activation Layers: tensor([0, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected2 Softmax Layers: tensor([0, 3, 8, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 11]), Loss: 0.3709379193931818
RL Action 14, Selected1 Activation Layers: tensor([ 2,  5,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  4,  7, 11]), Selected2 Softmax Layers: tensor([2, 5, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  6, 10]), Loss: 0.3945774843543768
RL Action 13, Selected1 Activation Layers: tensor([ 0,  3,  6, 11]), Selected2 Activation Layers: tensor([2, 5, 8, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 1,  5,  8, 10]), Selected3 Softmax Layers: tensor([0, 3, 7, 9]), Loss: 0.3874302227795124
RL Action 15, Selected1 Activation Layers: tensor([2, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([ 0,  5,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected2 Softmax Layers: tensor([1, 3, 8, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 11]), Loss: 0.3761568005383015
RL Action 14, Selected1 Activation Layers: tensor([1, 5, 6, 9]), Selected2 Activation Layers: tensor([ 0,  3,  8, 10]), Selected3 Activation Layers: tensor([ 2,  4,  7, 11]),Selected1 Softmax Layers: tensor([ 2,  4,  7, 10]), Selected2 Softmax Layers: tensor([ 0,  5,  6, 11]), Selected3 Softmax Layers: tensor([1, 3, 8, 9]), Loss: 0.36920976370573044
RL Action 16, Selected1 Activation Layers: tensor([ 0,  4,  8, 10]), Selected2 Activation Layers: tensor([1, 5, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([0, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  8, 11]), Loss: 0.35630783513188363
RL Action 15, Selected1 Activation Layers: tensor([ 2,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  3,  6, 10]), Selected3 Activation Layers: tensor([1, 4, 8, 9]),Selected1 Softmax Layers: tensor([1, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected3 Softmax Layers: tensor([ 2,  4,  7, 11]), Loss: 0.3926398642361164
RL Action 17, Selected1 Activation Layers: tensor([ 2,  4,  7, 10]), Selected2 Activation Layers: tensor([0, 3, 6, 9]), Selected3 Activation Layers: tensor([ 1,  5,  8, 11]),Selected1 Softmax Layers: tensor([0, 4, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 11]), Loss: 0.3567059708014131
RL Action 16, Selected1 Activation Layers: tensor([1, 3, 8, 9]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected3 Softmax Layers: tensor([0, 4, 6, 9]), Loss: 0.3704120095819235
RL Action 18, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([ 0,  5,  8, 11]), Selected3 Activation Layers: tensor([2, 4, 7, 9]),Selected1 Softmax Layers: tensor([ 1,  4,  6, 10]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected3 Softmax Layers: tensor([0, 3, 7, 9]), Loss: 0.37391679987311366
RL Action 17, Selected1 Activation Layers: tensor([1, 3, 7, 9]), Selected2 Activation Layers: tensor([ 0,  4,  8, 11]), Selected3 Activation Layers: tensor([ 2,  5,  6, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  6, 11]), Selected2 Softmax Layers: tensor([2, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 0,  4,  8, 10]), Loss: 0.3644423415884376
RL Action 19, Selected1 Activation Layers: tensor([0, 3, 8, 9]), Selected2 Activation Layers: tensor([ 1,  4,  7, 11]), Selected3 Activation Layers: tensor([ 2,  5,  6, 10]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([1, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  8, 11]), Loss: 0.35037345077842474
RL Action 18, Selected1 Activation Layers: tensor([ 0,  5,  8, 10]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 6, 9]),Selected1 Softmax Layers: tensor([2, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 10]), Loss: 0.35436687551438806
RL Action 20, Selected1 Activation Layers: tensor([1, 5, 8, 9]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([ 2,  3,  7, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  6, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected3 Softmax Layers: tensor([0, 3, 7, 9]), Loss: 0.3706840795278549
RL Action 19, Selected1 Activation Layers: tensor([ 1,  5,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 2,  4,  7, 10]),Selected1 Softmax Layers: tensor([0, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([ 2,  4,  6, 11]), Loss: 0.3684694587625563
RL Action 21, Selected1 Activation Layers: tensor([ 2,  4,  8, 11]), Selected2 Activation Layers: tensor([1, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  3,  6, 10]),Selected1 Softmax Layers: tensor([2, 4, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 10]), Loss: 0.37500250928103923
RL Action 20, Selected1 Activation Layers: tensor([ 2,  5,  6, 10]), Selected2 Activation Layers: tensor([1, 3, 8, 9]), Selected3 Activation Layers: tensor([ 0,  4,  7, 11]),Selected1 Softmax Layers: tensor([ 0,  5,  8, 11]), Selected2 Softmax Layers: tensor([1, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 10]), Loss: 0.3767769193276763
RL Action 22, Selected1 Activation Layers: tensor([2, 5, 6, 9]), Selected2 Activation Layers: tensor([ 1,  4,  8, 10]), Selected3 Activation Layers: tensor([ 0,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  5,  6, 10]), Selected3 Softmax Layers: tensor([2, 3, 8, 9]), Loss: 0.3696504229307175
RL Action 21, Selected1 Activation Layers: tensor([0, 4, 8, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 10]), Loss: 0.4030258198082447
RL Action 23, Selected1 Activation Layers: tensor([ 0,  3,  8, 11]), Selected2 Activation Layers: tensor([ 2,  4,  6, 10]), Selected3 Activation Layers: tensor([1, 5, 7, 9]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  8, 10]), Loss: 0.3790073378011584
All action Losses: [0.3844630786776543, 0.33954415649175645, 0.4082217642664909, 0.37712348394095896, 0.3798182413727045, 0.3555246302485466, 0.36382564313709737, 0.38781238287687303, 0.34295790441334245, 0.41144802555441856, 0.3420825655385852, 0.40587564527988435, 0.3644373233988881, 0.40007352448999883, 0.3945774843543768, 0.3761568005383015, 0.35630783513188363, 0.3567059708014131, 0.37391679987311366, 0.35037345077842474, 0.3706840795278549, 0.37500250928103923, 0.3696504229307175, 0.3790073378011584]
Rewards: [-0.007602072303350305, 0.02367669754282864, -0.02358672844303722, -0.002586775557209653, -0.00443243684243444, 0.012387527825752587, 0.006594209292190234, -0.009878518085456367, 0.021249929763236586, -0.025728192019720653, 0.021871401785687694, -0.022025123671141622, 0.006169213941098706, -0.0181473921546651, -0.014453415315997975, -0.0019234731955816153, 0.011838868277109116, 0.011560126470488985, -0.00038400108598179994, 0.016006817474213286, 0.001843819942844216, -0.0011306008713839466, 0.0025576826679589715, -0.0038775654374465507]
Probs_Activation:tensor([0.5570, 0.5284, 0.4152, 0.4934, 0.4945, 0.5121, 0.5232, 0.4809, 0.4959,
        0.4865, 0.5138, 0.4997]), Probs_Softmax:tensor([0.4854, 0.5281, 0.4866, 0.5345, 0.5269, 0.4389, 0.4776, 0.5193, 0.5032,
        0.4795, 0.5121, 0.5085])
In the 2'th rl step, updating importance scores...
Current Activation Importance Scores: [0.1367829144001007, 0.31849512457847595, -0.4515017569065094, -0.30733489990234375, -0.034590497612953186, 0.34180301427841187, 0.4057779610157013, -0.29689347743988037, -0.10923101007938385, -0.24803508818149567, 0.08548790216445923, 0.16266633570194244]
Current Softmax Importance Scores: [-0.11497195810079575, 0.38127779960632324, -0.26695334911346436, 0.3293914496898651, 0.02643217332661152, -0.3548482358455658, -0.5317344665527344, 0.2824026644229889, 0.24990683794021606, -0.109194315969944, 0.14497677981853485, -0.03577321767807007]
RL Action 22, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 0,  3,  8, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([0, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 11]), Loss: 0.3585239006206393
RL Action 0, Selected1 Activation Layers: tensor([ 2,  4,  7, 11]), Selected2 Activation Layers: tensor([1, 5, 8, 9]), Selected3 Activation Layers: tensor([ 0,  3,  6, 10]),Selected1 Softmax Layers: tensor([ 0,  3,  8, 11]), Selected2 Softmax Layers: tensor([2, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 10]), Loss: 0.376768313832581
RL Action 23, Selected1 Activation Layers: tensor([1, 4, 7, 9]), Selected2 Activation Layers: tensor([ 0,  5,  6, 10]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 10]), Selected2 Softmax Layers: tensor([2, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 11]), Loss: 0.36783579502254726
All action Losses: [0.38177882868796587, 0.36105658136308194, 0.36833742674440145, 0.4128008569777012, 0.38512406203895805, 0.40471644192934036, 0.36915483240038155, 0.40625485695898533, 0.3757512001693249, 0.3656855836138129, 0.36288573473691943, 0.38280562199652196, 0.3709379193931818, 0.3874302227795124, 0.36920976370573044, 0.3926398642361164, 0.3704120095819235, 0.3644423415884376, 0.35436687551438806, 0.3684694587625563, 0.3767769193276763, 0.4030258198082447, 0.3585239006206393, 0.36783579502254726]
Rewards: [-0.003001606363152831, 0.011291938971883408, 0.006236057649196547, -0.023853563213322104, -0.005281401241293837, -0.01848166025600162, 0.0056707390928241175, -0.019507249309718144, 0.001125556250205717, 0.00807325954808158, 0.010018294818199158, -0.003702182991530578, 0.004439156646287468, -0.006848627282601716, 0.005632765115685001, -0.010375731228962515, 0.004802175504381223, 0.00893625885246474, 0.015969889256424064, 0.006144712880651992, 0.00042148099464622835, -0.017352780732792006, 0.013059301520981892, 0.006583215517460328]
Probs_Activation:tensor([0.5179, 0.4960, 0.4861, 0.4939, 0.5050, 0.5011, 0.4989, 0.4597, 0.5414,
        0.5059, 0.4669, 0.5273]), Probs_Softmax:tensor([0.4722, 0.5187, 0.5091, 0.5178, 0.5272, 0.4551, 0.4814, 0.5284, 0.4902,
        0.4851, 0.4852, 0.5297])
In the 2'th rl step, updating importance scores...
Current Activation Importance Scores: [0.20409207046031952, -0.03192420303821564, -0.1722463071346283, -0.14205534756183624, 0.07636629790067673, 0.06570059061050415, 0.14829997718334198, -0.35955560207366943, 0.21222925186157227, -0.11354164034128189, -0.20291568338871002, 0.31616830825805664]
Current Softmax Importance Scores: [-0.16302907466888428, 0.19974035024642944, -0.036701690405607224, 0.12646690011024475, 0.18617483973503113, -0.3118700087070465, -0.2927880883216858, 0.22625182569026947, 0.06643477082252502, -0.18393898010253906, -0.0716404840350151, 0.25521785020828247]
RL Action 1, Selected1 Activation Layers: tensor([ 0,  4,  7, 10]), Selected2 Activation Layers: tensor([2, 3, 8, 9]), Selected3 Activation Layers: tensor([ 1,  5,  6, 11]),Selected1 Softmax Layers: tensor([0, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  7, 10]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.36021091245114806
RL Action 0, Selected1 Activation Layers: tensor([ 2,  5,  8, 11]), Selected2 Activation Layers: tensor([ 0,  4,  7, 10]), Selected3 Activation Layers: tensor([1, 3, 6, 9]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.37902753561735153
RL Action 2, Selected1 Activation Layers: tensor([ 0,  3,  8, 11]), Selected2 Activation Layers: tensor([2, 4, 7, 9]), Selected3 Activation Layers: tensor([ 1,  5,  6, 10]),Selected1 Softmax Layers: tensor([1, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 0,  5,  6, 11]), Loss: 0.4029183830320835
RL Action 1, Selected1 Activation Layers: tensor([ 0,  5,  8, 11]), Selected2 Activation Layers: tensor([2, 4, 6, 9]), Selected3 Activation Layers: tensor([ 1,  3,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected2 Softmax Layers: tensor([1, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 0,  4,  8, 10]), Loss: 0.3757902285084128
RL Action 3, Selected1 Activation Layers: tensor([ 1,  3,  7, 10]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([1, 4, 8, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  6, 11]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 10]), Loss: 0.38671122305095196
RL Action 2, Selected1 Activation Layers: tensor([0, 3, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  7, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([0, 4, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 10]), Loss: 0.34152853101491926
RL Action 4, Selected1 Activation Layers: tensor([2, 5, 6, 9]), Selected2 Activation Layers: tensor([ 1,  4,  7, 10]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 1,  5,  7, 10]), Selected2 Softmax Layers: tensor([2, 3, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 11]), Loss: 0.348839086741209
RL Action 3, Selected1 Activation Layers: tensor([2, 3, 8, 9]), Selected2 Activation Layers: tensor([ 1,  4,  6, 11]), Selected3 Activation Layers: tensor([ 0,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected2 Softmax Layers: tensor([2, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 11]), Loss: 0.3635259936749935
RL Action 5, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  7, 11]), Selected3 Activation Layers: tensor([1, 5, 6, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected3 Softmax Layers: tensor([2, 4, 7, 9]), Loss: 0.39168837197124956
RL Action 4, Selected1 Activation Layers: tensor([ 0,  4,  7, 10]), Selected2 Activation Layers: tensor([ 2,  5,  8, 11]), Selected3 Activation Layers: tensor([1, 3, 6, 9]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 1,  5,  8, 10]), Loss: 0.37756092719733714
RL Action 6, Selected1 Activation Layers: tensor([1, 3, 6, 9]), Selected2 Activation Layers: tensor([ 2,  4,  8, 11]), Selected3 Activation Layers: tensor([ 0,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  7, 10]), Selected3 Softmax Layers: tensor([0, 5, 6, 9]), Loss: 0.3810163028538227
RL Action 5, Selected1 Activation Layers: tensor([1, 3, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  7, 11]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected2 Softmax Layers: tensor([0, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 11]), Loss: 0.3776106494292617
RL Action 7, Selected1 Activation Layers: tensor([ 2,  5,  8, 10]), Selected2 Activation Layers: tensor([0, 4, 6, 9]), Selected3 Activation Layers: tensor([ 1,  3,  7, 11]),Selected1 Softmax Layers: tensor([2, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 10]), Loss: 0.36892162419855595
RL Action 6, Selected1 Activation Layers: tensor([ 2,  5,  7, 10]), Selected2 Activation Layers: tensor([ 1,  3,  8, 11]), Selected3 Activation Layers: tensor([0, 4, 6, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([1, 3, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 11]), Loss: 0.37302944142371414
RL Action 8, Selected1 Activation Layers: tensor([ 2,  3,  8, 10]), Selected2 Activation Layers: tensor([1, 5, 6, 9]), Selected3 Activation Layers: tensor([ 0,  4,  7, 11]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 11]), Selected2 Softmax Layers: tensor([2, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 10]), Loss: 0.36930415488779544
RL Action 7, Selected1 Activation Layers: tensor([ 1,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  8, 10]), Selected3 Activation Layers: tensor([2, 3, 6, 9]),Selected1 Softmax Layers: tensor([2, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 11]), Loss: 0.3871921208500862
RL Action 9, Selected1 Activation Layers: tensor([ 0,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([2, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 0,  5,  6, 11]), Loss: 0.4101615741848946
RL Action 8, Selected1 Activation Layers: tensor([1, 4, 7, 9]), Selected2 Activation Layers: tensor([ 2,  3,  8, 10]), Selected3 Activation Layers: tensor([ 0,  5,  6, 11]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.3736291391402483
RL Action 10, Selected1 Activation Layers: tensor([0, 4, 7, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 11]), Selected3 Activation Layers: tensor([ 1,  3,  8, 10]),Selected1 Softmax Layers: tensor([0, 4, 8, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  6, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.36322022680193183
RL Action 9, Selected1 Activation Layers: tensor([1, 5, 8, 9]), Selected2 Activation Layers: tensor([ 2,  3,  7, 10]), Selected3 Activation Layers: tensor([ 0,  4,  6, 11]),Selected1 Softmax Layers: tensor([2, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.35748922131955624
RL Action 11, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([0, 5, 8, 9]), Selected3 Activation Layers: tensor([ 2,  4,  7, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 10]), Selected3 Softmax Layers: tensor([0, 5, 8, 9]), Loss: 0.3738731060922146
RL Action 10, Selected1 Activation Layers: tensor([2, 3, 6, 9]), Selected2 Activation Layers: tensor([ 1,  4,  7, 11]), Selected3 Activation Layers: tensor([ 0,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  6, 10]), Selected3 Softmax Layers: tensor([1, 4, 7, 9]), Loss: 0.34412162613123654
RL Action 12, Selected1 Activation Layers: tensor([2, 5, 8, 9]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([ 2,  3,  7, 11]), Selected3 Softmax Layers: tensor([1, 4, 8, 9]), Loss: 0.3753241622820497
RL Action 11, Selected1 Activation Layers: tensor([ 0,  3,  8, 11]), Selected2 Activation Layers: tensor([ 1,  5,  6, 10]), Selected3 Activation Layers: tensor([2, 4, 7, 9]),Selected1 Softmax Layers: tensor([ 1,  5,  7, 10]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected3 Softmax Layers: tensor([2, 3, 6, 9]), Loss: 0.39371633902192116
RL Action 13, Selected1 Activation Layers: tensor([ 0,  3,  7, 10]), Selected2 Activation Layers: tensor([2, 5, 8, 9]), Selected3 Activation Layers: tensor([ 1,  4,  6, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 1,  5,  8, 10]), Selected3 Softmax Layers: tensor([2, 3, 7, 9]), Loss: 0.34343098297715186
RL Action 12, Selected1 Activation Layers: tensor([2, 4, 8, 9]), Selected2 Activation Layers: tensor([ 1,  5,  7, 11]), Selected3 Activation Layers: tensor([ 0,  3,  6, 10]),Selected1 Softmax Layers: tensor([2, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  6, 11]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 10]), Loss: 0.37531950198113917
RL Action 14, Selected1 Activation Layers: tensor([1, 4, 6, 9]), Selected2 Activation Layers: tensor([ 0,  3,  8, 11]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected2 Softmax Layers: tensor([ 1,  5,  8, 11]), Selected3 Softmax Layers: tensor([2, 3, 6, 9]), Loss: 0.39126337446272375
RL Action 13, Selected1 Activation Layers: tensor([ 2,  3,  6, 10]), Selected2 Activation Layers: tensor([0, 4, 8, 9]), Selected3 Activation Layers: tensor([ 1,  5,  7, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  6, 10]), Loss: 0.37498520910739896
RL Action 15, Selected1 Activation Layers: tensor([2, 4, 8, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 11]), Selected3 Activation Layers: tensor([ 1,  5,  7, 10]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 1,  5,  7, 10]), Selected3 Softmax Layers: tensor([ 0,  4,  8, 11]), Loss: 0.3758062093704939
RL Action 14, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([ 2,  4,  8, 11]), Selected3 Activation Layers: tensor([0, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  3,  7, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 10]), Selected3 Softmax Layers: tensor([1, 5, 8, 9]), Loss: 0.37721779476851225
RL Action 16, Selected1 Activation Layers: tensor([ 0,  5,  7, 11]), Selected2 Activation Layers: tensor([2, 4, 6, 9]), Selected3 Activation Layers: tensor([ 1,  3,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected2 Softmax Layers: tensor([ 1,  3,  7, 11]), Selected3 Softmax Layers: tensor([0, 5, 6, 9]), Loss: 0.42262697756290435
RL Action 15, Selected1 Activation Layers: tensor([ 2,  5,  7, 11]), Selected2 Activation Layers: tensor([1, 3, 8, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.37458311948925255
RL Action 17, Selected1 Activation Layers: tensor([ 0,  5,  6, 11]), Selected2 Activation Layers: tensor([ 2,  3,  7, 10]), Selected3 Activation Layers: tensor([1, 4, 8, 9]),Selected1 Softmax Layers: tensor([0, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 10]), Loss: 0.3668958140164614
RL Action 16, Selected1 Activation Layers: tensor([ 1,  5,  8, 10]), Selected2 Activation Layers: tensor([0, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  6, 11]),Selected1 Softmax Layers: tensor([1, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 0,  5,  6, 11]), Loss: 0.4213785268366337
RL Action 18, Selected1 Activation Layers: tensor([ 2,  3,  7, 11]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 0,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 1,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 0,  5,  7, 10]), Selected3 Softmax Layers: tensor([2, 3, 6, 9]), Loss: 0.36506387289613484
RL Action 17, Selected1 Activation Layers: tensor([ 2,  3,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([1, 5, 8, 9]),Selected1 Softmax Layers: tensor([0, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  4,  6, 11]), Loss: 0.3743912080302835
RL Action 19, Selected1 Activation Layers: tensor([ 2,  3,  8, 11]), Selected2 Activation Layers: tensor([ 1,  5,  7, 10]), Selected3 Activation Layers: tensor([0, 4, 6, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  6, 11]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 10]), Loss: 0.3685683828219771
RL Action 18, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 1,  4,  8, 11]), Selected3 Activation Layers: tensor([ 0,  3,  6, 10]),Selected1 Softmax Layers: tensor([ 1,  4,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 10]), Loss: 0.3779085973650217
RL Action 20, Selected1 Activation Layers: tensor([ 0,  4,  6, 10]), Selected2 Activation Layers: tensor([ 2,  5,  8, 11]), Selected3 Activation Layers: tensor([1, 3, 7, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  6, 10]), Selected2 Softmax Layers: tensor([ 1,  4,  8, 11]), Selected3 Softmax Layers: tensor([0, 5, 7, 9]), Loss: 0.3775111325830221
RL Action 19, Selected1 Activation Layers: tensor([0, 4, 6, 9]), Selected2 Activation Layers: tensor([ 2,  3,  8, 11]), Selected3 Activation Layers: tensor([ 1,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 11]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  8, 10]), Loss: 0.36027707494795325
RL Action 21, Selected1 Activation Layers: tensor([ 1,  3,  8, 10]), Selected2 Activation Layers: tensor([2, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 11]),Selected1 Softmax Layers: tensor([1, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 10]), Loss: 0.3409662738069892
RL Action 20, Selected1 Activation Layers: tensor([ 2,  4,  8, 10]), Selected2 Activation Layers: tensor([1, 5, 6, 9]), Selected3 Activation Layers: tensor([ 0,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 2,  3,  7, 10]), Selected3 Softmax Layers: tensor([0, 5, 8, 9]), Loss: 0.3887101975828409
RL Action 22, Selected1 Activation Layers: tensor([ 0,  5,  8, 11]), Selected2 Activation Layers: tensor([2, 3, 6, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 10]),Selected1 Softmax Layers: tensor([2, 4, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  7, 11]), Selected3 Softmax Layers: tensor([ 1,  5,  8, 10]), Loss: 0.37299974538385866
RL Action 21, Selected1 Activation Layers: tensor([2, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  5,  8, 11]), Selected3 Activation Layers: tensor([ 0,  3,  6, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  6, 10]), Selected2 Softmax Layers: tensor([2, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.35829981081187723
RL Action 23, Selected1 Activation Layers: tensor([0, 3, 8, 9]), Selected2 Activation Layers: tensor([ 2,  4,  7, 10]), Selected3 Activation Layers: tensor([ 1,  5,  6, 11]),Selected1 Softmax Layers: tensor([2, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 11]), Loss: 0.3452679992467165
All action Losses: [0.376768313832581, 0.36021091245114806, 0.4029183830320835, 0.38671122305095196, 0.348839086741209, 0.39168837197124956, 0.3810163028538227, 0.36892162419855595, 0.36930415488779544, 0.4101615741848946, 0.36322022680193183, 0.3738731060922146, 0.3753241622820497, 0.34343098297715186, 0.39126337446272375, 0.3758062093704939, 0.42262697756290435, 0.3668958140164614, 0.36506387289613484, 0.3685683828219771, 0.3775111325830221, 0.3409662738069892, 0.37299974538385866, 0.3452679992467165]
Rewards: [-0.0019392977947760848, 0.009514885595622746, -0.019647660211303153, -0.008727078268709887, 0.01749233914214776, -0.012099592253431157, -0.00484755540659787, 0.0034652962266629173, 0.00320083464286558, -0.024471277317685503, 0.007418956222040807, 4.9910078929626955e-05, -0.0009477857314008631, 0.021318128099951816, -0.011812269132050912, -0.0012789043339322959, -0.03269126964662972, 0.00486752249090705, 0.006138004575427014, 0.0037095985800222886, -0.0024487379418245414, 0.02306858255277744, 0.0006511008212460867, 0.020016269009739518]
Probs_Activation:tensor([0.5341, 0.5790, 0.3890, 0.4238, 0.4914, 0.5846, 0.6001, 0.4263, 0.4727,
        0.4383, 0.5214, 0.5406]), Probs_Softmax:tensor([0.4713, 0.5942, 0.4337, 0.5816, 0.5066, 0.4122, 0.3701, 0.5701, 0.5622,
        0.4727, 0.5362, 0.4911])
In the 3'th rl step, updating importance scores...
Current Activation Importance Scores: [0.31954270601272583, 0.3217966854572296, -0.6292881965637207, -0.46692031621932983, 0.1065254658460617, 0.3633917570114136, 0.5010325908660889, -0.27141252160072327, -0.23413442075252533, -0.3483329117298126, -0.09375350177288055, 0.44223034381866455]
Current Softmax Importance Scores: [-0.3184421956539154, 0.47837474942207336, -0.16530054807662964, 0.5846711993217468, 0.18246516585350037, -0.7602783441543579, -0.8541772961616516, 0.4702901244163513, 0.4016439616680145, -0.23935310542583466, 0.29096901416778564, -0.05198158696293831]
RL Action 22, Selected1 Activation Layers: tensor([1, 4, 8, 9]), Selected2 Activation Layers: tensor([ 2,  3,  6, 10]), Selected3 Activation Layers: tensor([ 0,  5,  7, 11]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected2 Softmax Layers: tensor([1, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 10]), Loss: 0.382207996584475
RL Action 0, Selected1 Activation Layers: tensor([ 2,  3,  6, 10]), Selected2 Activation Layers: tensor([1, 4, 7, 9]), Selected3 Activation Layers: tensor([ 0,  5,  8, 11]),Selected1 Softmax Layers: tensor([1, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 2,  4,  7, 10]), Loss: 0.34416172087192537
RL Action 23, Selected1 Activation Layers: tensor([0, 5, 7, 9]), Selected2 Activation Layers: tensor([ 2,  3,  6, 11]), Selected3 Activation Layers: tensor([ 1,  4,  8, 10]),Selected1 Softmax Layers: tensor([0, 4, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  3,  7, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 10]), Loss: 0.3657063563913107
All action Losses: [0.37902753561735153, 0.3757902285084128, 0.34152853101491926, 0.3635259936749935, 0.37756092719733714, 0.3776106494292617, 0.37302944142371414, 0.3871921208500862, 0.3736291391402483, 0.35748922131955624, 0.34412162613123654, 0.39371633902192116, 0.37531950198113917, 0.37498520910739896, 0.37721779476851225, 0.37458311948925255, 0.4213785268366337, 0.3743912080302835, 0.3779085973650217, 0.36027707494795325, 0.3887101975828409, 0.35829981081187723, 0.382207996584475, 0.3657063563913107]
Rewards: [-0.003558778570904475, -0.0013391643651065044, 0.022597648707235574, 0.007135113530871373, -0.0025541093076534294, -0.0025881946131410816, 0.0005594157468571392, -0.009124902210484898, 0.00014656074460150936, 0.01134469572648078, 0.02075716689937579, -0.013540170806009888, -0.0010158185298322353, -0.0007860976225846361, -0.002318840880274786, -0.0005096860840173578, -0.03194385495146168, -0.00037771973581313123, -0.0027924067089999394, 0.00939750210666801, -0.01015483460437705, 0.010777974645185817, -0.005732430783018105, 0.005620931666404427]
Probs_Activation:tensor([0.5508, 0.4920, 0.4570, 0.4645, 0.5191, 0.5164, 0.5370, 0.4111, 0.5529,
        0.4716, 0.4494, 0.5784]), Probs_Softmax:tensor([0.4593, 0.5498, 0.4908, 0.5316, 0.5464, 0.4227, 0.4273, 0.5563, 0.5166,
        0.4541, 0.4821, 0.5635])
In the 3'th rl step, updating importance scores...
Current Activation Importance Scores: [0.21751698851585388, 0.14107468724250793, -0.35747718811035156, -0.2680211365222931, 0.05309011787176132, 0.2154514640569687, 0.059511058032512665, -0.4057586193084717, 0.34768861532211304, -0.37909451127052307, 0.08223742246627808, 0.29501262307167053]
Current Softmax Importance Scores: [-0.278967022895813, 0.3865104615688324, -0.10860647261142731, 0.3465949594974518, 0.17642785608768463, -0.5179948210716248, -0.48032212257385254, 0.4206744134426117, 0.0611012764275074, -0.18507882952690125, -0.008920022286474705, 0.1945594698190689]
RL Action 1, Selected1 Activation Layers: tensor([2, 5, 6, 9]), Selected2 Activation Layers: tensor([ 0,  3,  7, 10]), Selected3 Activation Layers: tensor([ 1,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected2 Softmax Layers: tensor([1, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 10]), Loss: 0.3621512233093381
RL Action 0, Selected1 Activation Layers: tensor([ 2,  3,  7, 11]), Selected2 Activation Layers: tensor([1, 5, 6, 9]), Selected3 Activation Layers: tensor([ 0,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  8, 11]), Selected2 Softmax Layers: tensor([0, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  4,  6, 10]), Loss: 0.3562456786632538
RL Action 2, Selected1 Activation Layers: tensor([ 2,  5,  8, 10]), Selected2 Activation Layers: tensor([0, 4, 7, 9]), Selected3 Activation Layers: tensor([ 1,  3,  6, 11]),Selected1 Softmax Layers: tensor([0, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 10]), Loss: 0.35491844920441507
RL Action 1, Selected1 Activation Layers: tensor([ 2,  5,  6, 11]), Selected2 Activation Layers: tensor([0, 4, 7, 9]), Selected3 Activation Layers: tensor([ 1,  3,  8, 10]),Selected1 Softmax Layers: tensor([0, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.3557808605581522
RL Action 3, Selected1 Activation Layers: tensor([ 2,  3,  6, 11]), Selected2 Activation Layers: tensor([0, 5, 7, 9]), Selected3 Activation Layers: tensor([ 1,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.3471166730299592
RL Action 2, Selected1 Activation Layers: tensor([ 1,  5,  6, 10]), Selected2 Activation Layers: tensor([0, 4, 8, 9]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  5,  7, 10]), Selected2 Softmax Layers: tensor([ 0,  4,  6, 11]), Selected3 Softmax Layers: tensor([2, 3, 8, 9]), Loss: 0.3751995647698641
RL Action 4, Selected1 Activation Layers: tensor([ 2,  3,  7, 10]), Selected2 Activation Layers: tensor([ 1,  5,  6, 11]), Selected3 Activation Layers: tensor([0, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  4,  7, 10]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([0, 3, 8, 9]), Loss: 0.3602777697890997
RL Action 3, Selected1 Activation Layers: tensor([ 0,  4,  8, 11]), Selected2 Activation Layers: tensor([ 2,  5,  6, 10]), Selected3 Activation Layers: tensor([1, 3, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([1, 3, 8, 9]), Selected3 Softmax Layers: tensor([ 2,  4,  7, 11]), Loss: 0.3702521775662899
RL Action 5, Selected1 Activation Layers: tensor([ 1,  5,  6, 10]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 2,  4,  7, 11]),Selected1 Softmax Layers: tensor([2, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  7, 11]), Selected3 Softmax Layers: tensor([ 1,  3,  8, 10]), Loss: 0.347917261198163
RL Action 4, Selected1 Activation Layers: tensor([0, 3, 6, 9]), Selected2 Activation Layers: tensor([ 2,  5,  8, 11]), Selected3 Activation Layers: tensor([ 1,  4,  7, 10]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 11]), Loss: 0.37877394411712884
RL Action 6, Selected1 Activation Layers: tensor([ 1,  5,  8, 11]), Selected2 Activation Layers: tensor([2, 4, 7, 9]), Selected3 Activation Layers: tensor([ 0,  3,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  6, 10]), Selected2 Softmax Layers: tensor([ 0,  3,  7, 11]), Selected3 Softmax Layers: tensor([1, 5, 8, 9]), Loss: 0.37478264555335045
RL Action 5, Selected1 Activation Layers: tensor([ 1,  4,  6, 10]), Selected2 Activation Layers: tensor([ 2,  3,  7, 11]), Selected3 Activation Layers: tensor([0, 5, 8, 9]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  3,  6, 10]), Selected3 Softmax Layers: tensor([ 2,  4,  7, 11]), Loss: 0.409118908829987
RL Action 7, Selected1 Activation Layers: tensor([1, 3, 7, 9]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([ 0,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 10]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  8, 11]), Loss: 0.34956255808472636
RL Action 6, Selected1 Activation Layers: tensor([ 2,  4,  7, 11]), Selected2 Activation Layers: tensor([0, 5, 8, 9]), Selected3 Activation Layers: tensor([ 1,  3,  6, 10]),Selected1 Softmax Layers: tensor([ 0,  4,  6, 11]), Selected2 Softmax Layers: tensor([1, 5, 8, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  7, 10]), Loss: 0.3518714420124888
RL Action 8, Selected1 Activation Layers: tensor([ 2,  5,  7, 10]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([0, 4, 8, 9]),Selected1 Softmax Layers: tensor([0, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.36625735137611626
RL Action 7, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([2, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  3,  8, 10]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([2, 5, 7, 9]), Loss: 0.38712738916277883
RL Action 9, Selected1 Activation Layers: tensor([1, 3, 6, 9]), Selected2 Activation Layers: tensor([ 0,  5,  7, 10]), Selected3 Activation Layers: tensor([ 2,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected2 Softmax Layers: tensor([0, 5, 8, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 10]), Loss: 0.3685519420728087
RL Action 8, Selected1 Activation Layers: tensor([2, 3, 6, 9]), Selected2 Activation Layers: tensor([ 0,  5,  7, 10]), Selected3 Activation Layers: tensor([ 1,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  6, 10]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 11]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.3598081313446164
RL Action 10, Selected1 Activation Layers: tensor([ 0,  4,  6, 10]), Selected2 Activation Layers: tensor([2, 3, 8, 9]), Selected3 Activation Layers: tensor([ 1,  5,  7, 11]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  6, 11]), Loss: 0.37830037739127875
RL Action 9, Selected1 Activation Layers: tensor([2, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([ 0,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  8, 10]), Selected3 Softmax Layers: tensor([1, 4, 6, 9]), Loss: 0.37585635848343374
RL Action 11, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  7, 11]), Selected3 Activation Layers: tensor([1, 5, 6, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 11]), Selected2 Softmax Layers: tensor([1, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  4,  8, 10]), Loss: 0.37884969945997
RL Action 10, Selected1 Activation Layers: tensor([ 0,  3,  6, 11]), Selected2 Activation Layers: tensor([ 1,  4,  7, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  7, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([0, 4, 8, 9]), Loss: 0.40204180978238585
RL Action 12, Selected1 Activation Layers: tensor([0, 3, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 10]), Selected3 Activation Layers: tensor([ 1,  4,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  5,  8, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  6, 10]), Selected3 Softmax Layers: tensor([2, 4, 7, 9]), Loss: 0.37669102296233176
RL Action 11, Selected1 Activation Layers: tensor([ 1,  5,  6, 11]), Selected2 Activation Layers: tensor([2, 3, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([0, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 10]), Loss: 0.4285543954372406
RL Action 13, Selected1 Activation Layers: tensor([0, 5, 7, 9]), Selected2 Activation Layers: tensor([ 2,  3,  8, 11]), Selected3 Activation Layers: tensor([ 1,  4,  6, 10]),Selected1 Softmax Layers: tensor([2, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 10]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.35757955849170686
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
RL Action 12, Selected1 Activation Layers: tensor([1, 4, 6, 9]), Selected2 Activation Layers: tensor([ 2,  5,  7, 10]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([2, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  5,  7, 10]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 11]), Loss: 0.3838302146270871
RL Action 14, Selected1 Activation Layers: tensor([2, 3, 7, 9]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([ 1,  5,  8, 10]),Selected1 Softmax Layers: tensor([0, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  6, 11]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.35452336732298134
RL Action 13, Selected1 Activation Layers: tensor([0, 3, 7, 9]), Selected2 Activation Layers: tensor([ 1,  5,  6, 10]), Selected3 Activation Layers: tensor([ 2,  4,  8, 11]),Selected1 Softmax Layers: tensor([1, 4, 8, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  6, 10]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 11]), Loss: 0.36411221213638784
RL Action 15, Selected1 Activation Layers: tensor([2, 4, 8, 9]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([ 0,  5,  6, 10]),Selected1 Softmax Layers: tensor([0, 4, 6, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 11]), Loss: 0.353925998210907
RL Action 14, Selected1 Activation Layers: tensor([ 0,  5,  8, 11]), Selected2 Activation Layers: tensor([2, 4, 7, 9]), Selected3 Activation Layers: tensor([ 1,  3,  6, 10]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 10]), Selected2 Softmax Layers: tensor([ 1,  3,  6, 11]), Selected3 Softmax Layers: tensor([2, 4, 8, 9]), Loss: 0.37997466553002596
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
RL Action 16, Selected1 Activation Layers: tensor([ 2,  3,  8, 11]), Selected2 Activation Layers: tensor([ 1,  5,  6, 10]), Selected3 Activation Layers: tensor([0, 4, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 11]), Selected2 Softmax Layers: tensor([2, 3, 8, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.34532783534377814
RL Action 15, Selected1 Activation Layers: tensor([ 1,  4,  8, 10]), Selected2 Activation Layers: tensor([ 0,  3,  7, 11]), Selected3 Activation Layers: tensor([2, 5, 6, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  8, 11]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  7, 10]), Loss: 0.3716599727794528
RL Action 17, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 1,  4,  6, 10]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected2 Softmax Layers: tensor([1, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  6, 11]), Loss: 0.40566630639135837
RL Action 16, Selected1 Activation Layers: tensor([ 2,  3,  8, 10]), Selected2 Activation Layers: tensor([ 0,  5,  6, 11]), Selected3 Activation Layers: tensor([1, 4, 7, 9]),Selected1 Softmax Layers: tensor([ 1,  4,  8, 10]), Selected2 Softmax Layers: tensor([ 0,  5,  6, 11]), Selected3 Softmax Layers: tensor([2, 3, 7, 9]), Loss: 0.3722110802680254
RL Action 18, Selected1 Activation Layers: tensor([1, 5, 8, 9]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  3,  6, 10]), Selected2 Softmax Layers: tensor([0, 4, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  5,  8, 11]), Loss: 0.39642362125217917
RL Action 17, Selected1 Activation Layers: tensor([ 2,  5,  8, 11]), Selected2 Activation Layers: tensor([1, 3, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 1,  5,  8, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 2,  4,  7, 11]), Loss: 0.3766926433891058
RL Action 19, Selected1 Activation Layers: tensor([ 1,  4,  6, 10]), Selected2 Activation Layers: tensor([2, 3, 7, 9]), Selected3 Activation Layers: tensor([ 0,  5,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([2, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 10]), Loss: 0.335184229016304
RL Action 18, Selected1 Activation Layers: tensor([1, 3, 7, 9]), Selected2 Activation Layers: tensor([ 2,  4,  8, 10]), Selected3 Activation Layers: tensor([ 0,  5,  6, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  7, 10]), Selected2 Softmax Layers: tensor([ 1,  5,  8, 11]), Selected3 Softmax Layers: tensor([0, 4, 6, 9]), Loss: 0.3881991972029209
RL Action 20, Selected1 Activation Layers: tensor([ 1,  4,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([1, 5, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected3 Softmax Layers: tensor([ 0,  4,  6, 10]), Loss: 0.39281879298388955
RL Action 19, Selected1 Activation Layers: tensor([ 0,  5,  6, 10]), Selected2 Activation Layers: tensor([ 1,  4,  7, 11]), Selected3 Activation Layers: tensor([2, 3, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  4,  7, 10]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([0, 3, 8, 9]), Loss: 0.3696053513139486
RL Action 21, Selected1 Activation Layers: tensor([1, 5, 7, 9]), Selected2 Activation Layers: tensor([ 2,  3,  6, 11]), Selected3 Activation Layers: tensor([ 0,  4,  8, 10]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  7, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  6, 10]), Loss: 0.37196753431111573
RL Action 20, Selected1 Activation Layers: tensor([2, 5, 8, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 11]), Selected3 Activation Layers: tensor([ 1,  4,  7, 10]),Selected1 Softmax Layers: tensor([2, 5, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  7, 11]), Loss: 0.3475618866458535
RL Action 22, Selected1 Activation Layers: tensor([ 0,  4,  6, 11]), Selected2 Activation Layers: tensor([ 1,  3,  7, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  6, 11]), Selected2 Softmax Layers: tensor([ 0,  5,  8, 10]), Selected3 Softmax Layers: tensor([1, 4, 7, 9]), Loss: 0.3934417798370123
RL Action 21, Selected1 Activation Layers: tensor([1, 5, 8, 9]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([ 0,  3,  7, 10]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([2, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 10]), Loss: 0.37087559591978786
RL Action 23, Selected1 Activation Layers: tensor([ 1,  3,  7, 10]), Selected2 Activation Layers: tensor([ 2,  5,  8, 11]), Selected3 Activation Layers: tensor([0, 4, 6, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected2 Softmax Layers: tensor([0, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 11]), Loss: 0.36795799508690835
All action Losses: [0.34416172087192537, 0.3621512233093381, 0.35491844920441507, 0.3471166730299592, 0.3602777697890997, 0.347917261198163, 0.37478264555335045, 0.34956255808472636, 0.36625735137611626, 0.3685519420728087, 0.37830037739127875, 0.37884969945997, 0.37669102296233176, 0.35757955849170686, 0.35452336732298134, 0.353925998210907, 0.34532783534377814, 0.40566630639135837, 0.39642362125217917, 0.335184229016304, 0.39281879298388955, 0.37196753431111573, 0.3934417798370123, 0.36795799508690835]
Rewards: [0.01520863549480056, 0.0025714284585645064, 0.007624973571991545, 0.013117214728681659, 0.0038769063571940654, 0.01255164718595203, -0.006166972812552629, 0.011390764038759493, -0.0002813029475633444, -0.0018703747196128218, -0.008580949394807247, -0.008957145246415665, -0.007477614263265653, 0.005761402920824499, 0.00790207182198821, 0.00832125606020262, 0.014382558638572074, -0.02707310524982043, -0.02088399684357034, 0.021600659238877218, -0.01845457460581157, -0.004229029972477205, -0.01887505386153654, -0.0014593985989740377]
Probs_Activation:tensor([0.5792, 0.5798, 0.3477, 0.3853, 0.5266, 0.5899, 0.6227, 0.4326, 0.4417,
        0.4138, 0.4766, 0.6088]), Probs_Softmax:tensor([0.4211, 0.6174, 0.4588, 0.6421, 0.5455, 0.3186, 0.2986, 0.6155, 0.5991,
        0.4404, 0.5722, 0.4870])
In the 4'th rl step, updating importance scores...
Current Activation Importance Scores: [0.49916908144950867, 0.3692638874053955, -0.8406236171722412, -0.6862775087356567, 0.23848101496696472, 0.45938894152641296, 0.4724997580051422, -0.2858859896659851, -0.1896439790725708, -0.3137094974517822, -0.15347687900066376, 0.4652565121650696]
Current Softmax Importance Scores: [-0.46436354517936707, 0.6678891777992249, -0.21586735546588898, 0.8617621064186096, 0.2071933150291443, -1.0436991453170776, -0.9759610891342163, 0.5971058011054993, 0.4126189649105072, -0.3735884726047516, 0.5400692224502563, -0.1701442301273346]
RL Action 22, Selected1 Activation Layers: tensor([ 0,  4,  8, 10]), Selected2 Activation Layers: tensor([1, 3, 6, 9]), Selected3 Activation Layers: tensor([ 2,  5,  7, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected2 Softmax Layers: tensor([2, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  5,  8, 11]), Loss: 0.37747021578252316
RL Action 0, Selected1 Activation Layers: tensor([2, 3, 8, 9]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([ 1,  5,  7, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected2 Softmax Layers: tensor([1, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 0,  5,  6, 10]), Loss: 0.36876709669828417
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8804623788168142, spearman: 0.8763303317439618, Total Loss: 0.5261714058987638
Evaluating best combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 6, 9]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [1, 4, 8, 11]; Final Softmax Layer1: [0, 5, 6, 9]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [2, 3, 8, 11];
Final Metrics - pearson: 0.8733584565514843, spearman: 0.8684845042723898, Total Loss: 0.5365554146627163
Evaluating worst combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 8, 11]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [2, 3, 6, 9]; Final Softmax Layer1: [2, 3, 8, 11]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [0, 5, 6, 9];
Final Metrics - pearson: 0.8655635790702416, spearman: 0.8636978113495689, Total Loss: 0.6574338528704136
Evaluating all 1:
Final Gelu Layer1:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8570564516719225, spearman: 0.8528390166208515, Total Loss: 0.6221258375555911
Evaluating all 3:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
Final Metrics - pearson: 0.8782044338864387, spearman: 0.8739159243859494, Total Loss: 0.5415146350860596
Evaluating random1 combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 6, 10]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [2, 3, 8, 11]; Final Softmax Layer1: [0, 5, 8, 9]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [1, 3, 6, 11];
Final Metrics - pearson: 0.8733681298865793, spearman: 0.8685719153646638, Total Loss: 0.5474802009919857
Evaluating random2 combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 8, 11]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [1, 4, 6, 10]; Final Softmax Layer1: [1, 3, 6, 11]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [0, 5, 8, 9];
Final Metrics - pearson: 0.8698427897525359, spearman: 0.8665199406407896, Total Loss: 0.5859990115178392
Evaluating origin:
Final Gelu Layer1:[1, 2, 4, 6]; Final Gelu Layer2: [7, 3, 11, 10]; Final Gelu Layer3: [5, 0, 8, 9]; Final Softmax Layer1: [4, 10, 11, 7]; Final Softmax Layer2: [1, 2, 5, 8]; Final Softmax Layer3: [3, 6, 9, 0];
Final Metrics - pearson: 0.8704157827868897, spearman: 0.8669976544931721, Total Loss: 0.5902034015731609
In the 0'th rl step, updating importance scores...
Current Activation Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Current Softmax Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
RL Action 0, Selected1 Activation Layers: tensor([1, 4, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 11]), Selected3 Activation Layers: tensor([ 0,  3,  7, 10]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.39309333205223085
RL Action 1, Selected1 Activation Layers: tensor([ 1,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([2, 3, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([2, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.3609575401991606
RL Action 2, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.34891508918255565
RL Action 3, Selected1 Activation Layers: tensor([ 0,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([0, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  7, 10]), Loss: 0.3862261874601245
RL Action 4, Selected1 Activation Layers: tensor([ 0,  4,  7, 11]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([2, 4, 6, 9]), Loss: 0.3730655927211046
Model Structure: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
)
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
print bias: weights: torch.Size([768])
Per Layer QKV Spectral Norm: [{'Wqk_spectral_norm': [5.352284908294678, 3.261864185333252, 2.1756839752197266, 3.38678240776062, 2.4382381439208984, 2.271352767944336, 2.103235960006714, 2.7038822174072266, 4.299220085144043, 1.5204658508300781, 3.0804178714752197, 2.1938371658325195], 'Wv_spectral_norm': [1.1376709938049316, 1.1140375137329102, 1.0970133543014526, 0.9389089345932007, 1.1640889644622803, 1.115019679069519, 1.2574560642242432, 1.0974204540252686, 0.9675320386886597, 1.1525787115097046, 0.8864543437957764, 1.1506288051605225]}, {'Wqk_spectral_norm': [2.479400157928467, 4.57543420791626, 5.3105788230896, 2.425461769104004, 3.9026999473571777, 2.048389434814453, 6.460015773773193, 4.062199592590332, 2.5968551635742188, 1.4595496654510498, 3.951561212539673, 2.072615623474121], 'Wv_spectral_norm': [1.2686495780944824, 1.0295143127441406, 1.4319229125976562, 1.4721386432647705, 1.0030447244644165, 1.2234458923339844, 1.1239054203033447, 1.46125328540802, 1.2374823093414307, 1.4044249057769775, 1.2465026378631592, 1.2670657634735107]}, {'Wqk_spectral_norm': [29.8752498626709, 2.466578483581543, 1.5233668088912964, 1.5232107639312744, 1.7730112075805664, 5.480339527130127, 2.2129228115081787, 2.3829121589660645, 1.782092809677124, 29.673246383666992, 1.6669700145721436, 2.609942674636841], 'Wv_spectral_norm': [0.7638558149337769, 1.0952205657958984, 1.497760534286499, 1.5806962251663208, 1.1894676685333252, 1.2577754259109497, 1.1917574405670166, 1.2386326789855957, 1.3118085861206055, 0.7772070169448853, 1.2390685081481934, 1.2857074737548828]}, {'Wqk_spectral_norm': [7.265749454498291, 2.2480263710021973, 1.6461663246154785, 3.2285044193267822, 2.587027072906494, 8.309564590454102, 1.7154889106750488, 1.9840701818466187, 1.7254523038864136, 5.237835884094238, 2.0409553050994873, 3.109067916870117], 'Wv_spectral_norm': [1.228966474533081, 1.3376119136810303, 1.620136022567749, 1.3715403079986572, 1.4722933769226074, 1.154558777809143, 1.3778390884399414, 1.7600775957107544, 1.2116323709487915, 1.169581413269043, 1.0268663167953491, 1.2971314191818237]}, {'Wqk_spectral_norm': [2.1737565994262695, 4.4898576736450195, 2.1101343631744385, 3.976750373840332, 2.1871750354766846, 2.2912051677703857, 1.9985462427139282, 1.5243685245513916, 2.064235210418701, 1.6526463031768799, 2.257019519805908, 4.053987503051758], 'Wv_spectral_norm': [1.195576548576355, 1.3761403560638428, 1.7739887237548828, 1.4855108261108398, 1.4549833536148071, 1.2012946605682373, 1.2218222618103027, 1.1728066205978394, 1.3731701374053955, 1.5362000465393066, 1.2994425296783447, 1.305282711982727]}, {'Wqk_spectral_norm': [1.9790375232696533, 1.944514513015747, 2.025921583175659, 1.924391508102417, 1.5317240953445435, 1.4006500244140625, 1.6788625717163086, 1.6713148355484009, 2.019005298614502, 3.3116092681884766, 2.4799184799194336, 1.7942054271697998], 'Wv_spectral_norm': [1.386445164680481, 1.2456376552581787, 1.4507695436477661, 1.4344652891159058, 1.4180742502212524, 1.442568302154541, 1.059098720550537, 1.1765156984329224, 1.6198368072509766, 1.0217300653457642, 1.1565485000610352, 1.1782817840576172]}, {'Wqk_spectral_norm': [1.5376832485198975, 1.8733018636703491, 1.7050416469573975, 2.307241201400757, 1.7318522930145264, 1.6667428016662598, 2.205672264099121, 1.7221300601959229, 1.4426441192626953, 1.8353809118270874, 1.6601979732513428, 3.497267007827759], 'Wv_spectral_norm': [1.4394559860229492, 1.3942999839782715, 1.197433590888977, 1.1228634119033813, 1.0791497230529785, 1.0970971584320068, 1.2888420820236206, 1.7446374893188477, 1.5854485034942627, 1.0930702686309814, 1.0641937255859375, 0.9563988447189331]}, {'Wqk_spectral_norm': [1.8818929195404053, 1.4627859592437744, 2.975125312805176, 1.770932674407959, 3.399834156036377, 1.84869384765625, 2.1259288787841797, 2.5374464988708496, 1.5960273742675781, 1.7520021200180054, 1.6490721702575684, 2.738647937774658], 'Wv_spectral_norm': [1.4533798694610596, 1.3201929330825806, 1.2163652181625366, 1.0746515989303589, 0.8814424872398376, 1.206554889678955, 1.0953449010849, 1.1766793727874756, 1.3142948150634766, 1.2031216621398926, 1.0350933074951172, 1.0707132816314697]}, {'Wqk_spectral_norm': [2.370293617248535, 1.256226897239685, 2.0010900497436523, 2.161836624145508, 2.195359706878662, 2.8889801502227783, 3.193047285079956, 1.5884881019592285, 1.4878431558609009, 2.133542776107788, 2.1738224029541016, 1.9966330528259277], 'Wv_spectral_norm': [1.218674659729004, 1.482336401939392, 1.1116105318069458, 1.1054567098617554, 1.4542090892791748, 1.0317736864089966, 1.1792776584625244, 1.4717098474502563, 1.5722240209579468, 1.3196399211883545, 1.144007682800293, 1.0696396827697754]}, {'Wqk_spectral_norm': [2.012266159057617, 1.7871973514556885, 2.2822277545928955, 2.5477569103240967, 2.6057443618774414, 2.494777202606201, 3.0496959686279297, 1.9832289218902588, 1.695042371749878, 2.0533030033111572, 2.177032947540283, 1.6552233695983887], 'Wv_spectral_norm': [1.1409834623336792, 1.1437932252883911, 1.1670610904693604, 1.1456162929534912, 1.010146141052246, 1.4731038808822632, 1.2824651002883911, 0.9692022204399109, 1.3881304264068604, 1.0316907167434692, 1.2637519836425781, 1.549980640411377]}, {'Wqk_spectral_norm': [1.3868675231933594, 2.0475449562072754, 1.7593680620193481, 1.742780089378357, 2.2580909729003906, 2.898613929748535, 2.177516460418701, 2.056065320968628, 1.3233157396316528, 2.326497793197632, 2.5178112983703613, 2.633561849594116], 'Wv_spectral_norm': [1.5592485666275024, 1.2385497093200684, 1.4891273975372314, 1.5974760055541992, 1.1197459697723389, 1.3294484615325928, 1.1032204627990723, 1.3720738887786865, 1.0832445621490479, 1.2910771369934082, 0.8920152187347412, 1.3989561796188354]}, {'Wqk_spectral_norm': [1.9859983921051025, 2.708582639694214, 1.7245107889175415, 2.536527156829834, 1.8698101043701172, 2.1047229766845703, 1.9901283979415894, 2.28205943107605, 2.567025661468506, 2.3352513313293457, 2.2810840606689453, 2.4366660118103027], 'Wv_spectral_norm': [1.522117018699646, 1.5383355617523193, 1.7414854764938354, 1.8499101400375366, 1.2951469421386719, 1.216165542602539, 1.741443157196045, 1.2937641143798828, 1.5293447971343994, 1.5901751518249512, 1.643144130706787, 1.2519564628601074]}]
Evaluating origin:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8804623795719297, spearman: 0.8763303317439618, Total Loss: 0.5264316543619684
Evaluating best combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 6, 9]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [1, 4, 8, 11]; Final Softmax Layer1: [0, 5, 6, 9]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [2, 3, 8, 11];
Final Metrics - pearson: 0.8733585003456986, spearman: 0.8684845042723898, Total Loss: 0.5370658201740143
Evaluating worst combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 8, 11]; Final Gelu Layer2: [0, 5, 7, 10]; Final Gelu Layer3: [2, 3, 6, 9]; Final Softmax Layer1: [2, 3, 8, 11]; Final Softmax Layer2: [1, 4, 7, 10]; Final Softmax Layer3: [0, 5, 6, 9];
Final Metrics - pearson: 0.8655638381469283, spearman: 0.8636978113495689, Total Loss: 0.6570547720853318
Evaluating all 1:
Final Gelu Layer1:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Gelu Layer2: []; Final Gelu Layer3: []; Final Softmax Layer1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer2: []; Final Softmax Layer3: [];
Final Metrics - pearson: 0.8570564491040253, spearman: 0.8528390166208515, Total Loss: 0.6222240398538873
Evaluating all 3:
Final Gelu Layer1:[]; Final Gelu Layer2: []; Final Gelu Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]; Final Softmax Layer1: []; Final Softmax Layer2: []; Final Softmax Layer3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
Final Metrics - pearson: 0.8782043703640354, spearman: 0.8739155613963756, Total Loss: 0.5417816155451409
Evaluating random1 combination 4-4-4-lr15-group:
Final Gelu Layer1:[1, 4, 6, 10]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [2, 3, 8, 11]; Final Softmax Layer1: [0, 5, 8, 9]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [1, 3, 6, 11];
Final Metrics - pearson: 0.8733682998726445, spearman: 0.8685719153646638, Total Loss: 0.5478492429915894
Evaluating random2 combination 4-4-4-lr15-group:
Final Gelu Layer1:[2, 3, 8, 11]; Final Gelu Layer2: [0, 5, 7, 9]; Final Gelu Layer3: [1, 4, 6, 10]; Final Softmax Layer1: [1, 3, 6, 11]; Final Softmax Layer2: [2, 4, 7, 10]; Final Softmax Layer3: [0, 5, 8, 9];
Final Metrics - pearson: 0.8698426703911749, spearman: 0.8665204157300851, Total Loss: 0.5859625922872665
Evaluating origin:
Final Gelu Layer1:[1, 2, 4, 6]; Final Gelu Layer2: [7, 3, 11, 10]; Final Gelu Layer3: [5, 0, 8, 9]; Final Softmax Layer1: [4, 10, 11, 7]; Final Softmax Layer2: [1, 2, 5, 8]; Final Softmax Layer3: [3, 6, 9, 0];
Final Metrics - pearson: 0.8704158335028417, spearman: 0.8669976544931721, Total Loss: 0.5904375154604303
In the 0'th rl step, updating importance scores...
Current Activation Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Current Softmax Importance Scores: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
RL Action 0, Selected1 Activation Layers: tensor([1, 4, 8, 9]), Selected2 Activation Layers: tensor([ 2,  5,  6, 11]), Selected3 Activation Layers: tensor([ 0,  3,  7, 10]),Selected1 Softmax Layers: tensor([1, 5, 8, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  7, 10]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.37377314902842046
RL Action 1, Selected1 Activation Layers: tensor([ 1,  5,  7, 11]), Selected2 Activation Layers: tensor([ 0,  4,  6, 10]), Selected3 Activation Layers: tensor([2, 3, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  6, 10]), Selected2 Softmax Layers: tensor([2, 3, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.3686544838547707
RL Action 2, Selected1 Activation Layers: tensor([ 0,  3,  8, 10]), Selected2 Activation Layers: tensor([ 2,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 7, 9]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected3 Softmax Layers: tensor([1, 3, 7, 9]), Loss: 0.3812517645955086
RL Action 3, Selected1 Activation Layers: tensor([ 0,  5,  7, 10]), Selected2 Activation Layers: tensor([1, 4, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  8, 11]),Selected1 Softmax Layers: tensor([0, 3, 8, 9]), Selected2 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected3 Softmax Layers: tensor([ 2,  5,  7, 10]), Loss: 0.3999547792971134
RL Action 4, Selected1 Activation Layers: tensor([ 0,  4,  7, 11]), Selected2 Activation Layers: tensor([ 1,  3,  6, 10]), Selected3 Activation Layers: tensor([2, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 0,  5,  7, 11]), Selected2 Softmax Layers: tensor([ 1,  3,  8, 10]), Selected3 Softmax Layers: tensor([2, 4, 6, 9]), Loss: 0.38182611219584944
RL Action 5, Selected1 Activation Layers: tensor([ 1,  3,  6, 11]), Selected2 Activation Layers: tensor([0, 5, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  8, 10]),Selected1 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected2 Softmax Layers: tensor([ 2,  4,  6, 11]), Selected3 Softmax Layers: tensor([1, 5, 8, 9]), Loss: 0.3678722922503948
RL Action 6, Selected1 Activation Layers: tensor([ 2,  3,  8, 11]), Selected2 Activation Layers: tensor([1, 5, 7, 9]), Selected3 Activation Layers: tensor([ 0,  4,  6, 10]),Selected1 Softmax Layers: tensor([1, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  6, 11]), Selected3 Softmax Layers: tensor([ 0,  3,  8, 10]), Loss: 0.38261675730347633
RL Action 7, Selected1 Activation Layers: tensor([ 0,  4,  6, 11]), Selected2 Activation Layers: tensor([1, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([2, 3, 6, 9]), Selected2 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  5,  7, 11]), Loss: 0.38267640098929406
RL Action 8, Selected1 Activation Layers: tensor([ 0,  5,  8, 10]), Selected2 Activation Layers: tensor([2, 3, 6, 9]), Selected3 Activation Layers: tensor([ 1,  4,  7, 11]),Selected1 Softmax Layers: tensor([0, 3, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  4,  8, 11]), Selected3 Softmax Layers: tensor([ 1,  5,  6, 10]), Loss: 0.4038457218557596
RL Action 9, Selected1 Activation Layers: tensor([ 0,  5,  6, 10]), Selected2 Activation Layers: tensor([ 1,  3,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 1,  3,  8, 11]), Selected2 Softmax Layers: tensor([ 2,  4,  7, 10]), Selected3 Softmax Layers: tensor([0, 5, 6, 9]), Loss: 0.4108642582595348
RL Action 10, Selected1 Activation Layers: tensor([ 2,  5,  6, 10]), Selected2 Activation Layers: tensor([1, 4, 7, 9]), Selected3 Activation Layers: tensor([ 0,  3,  8, 11]),Selected1 Softmax Layers: tensor([ 0,  4,  8, 10]), Selected2 Softmax Layers: tensor([1, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 2,  3,  6, 11]), Loss: 0.36607388537377117
RL Action 11, Selected1 Activation Layers: tensor([ 1,  5,  8, 11]), Selected2 Activation Layers: tensor([0, 3, 7, 9]), Selected3 Activation Layers: tensor([ 2,  4,  6, 10]),Selected1 Softmax Layers: tensor([ 2,  5,  7, 10]), Selected2 Softmax Layers: tensor([0, 3, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  8, 11]), Loss: 0.3907616949826479
RL Action 12, Selected1 Activation Layers: tensor([2, 5, 7, 9]), Selected2 Activation Layers: tensor([ 0,  3,  6, 10]), Selected3 Activation Layers: tensor([ 1,  4,  8, 11]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 11]), Selected2 Softmax Layers: tensor([0, 5, 6, 9]), Selected3 Softmax Layers: tensor([ 1,  4,  7, 10]), Loss: 0.37966198720037936
RL Action 13, Selected1 Activation Layers: tensor([0, 4, 7, 9]), Selected2 Activation Layers: tensor([ 1,  3,  6, 11]), Selected3 Activation Layers: tensor([ 2,  5,  8, 10]),Selected1 Softmax Layers: tensor([ 2,  4,  8, 10]), Selected2 Softmax Layers: tensor([0, 5, 7, 9]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.38518938463181257
RL Action 14, Selected1 Activation Layers: tensor([ 1,  4,  8, 10]), Selected2 Activation Layers: tensor([0, 5, 6, 9]), Selected3 Activation Layers: tensor([ 2,  3,  7, 11]),Selected1 Softmax Layers: tensor([ 1,  4,  6, 11]), Selected2 Softmax Layers: tensor([ 0,  3,  7, 10]), Selected3 Softmax Layers: tensor([2, 5, 8, 9]), Loss: 0.39197240822017193
RL Action 15, Selected1 Activation Layers: tensor([ 2,  3,  7, 10]), Selected2 Activation Layers: tensor([ 0,  4,  6, 11]), Selected3 Activation Layers: tensor([1, 5, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  5,  6, 10]), Selected2 Softmax Layers: tensor([1, 4, 8, 9]), Selected3 Softmax Layers: tensor([ 0,  3,  7, 11]), Loss: 0.3522478279471397
RL Action 16, Selected1 Activation Layers: tensor([ 1,  4,  6, 11]), Selected2 Activation Layers: tensor([0, 3, 8, 9]), Selected3 Activation Layers: tensor([ 2,  5,  7, 10]),Selected1 Softmax Layers: tensor([0, 4, 7, 9]), Selected2 Softmax Layers: tensor([ 2,  5,  8, 10]), Selected3 Softmax Layers: tensor([ 1,  3,  6, 11]), Loss: 0.38077472791075706
RL Action 17, Selected1 Activation Layers: tensor([ 1,  3,  6, 10]), Selected2 Activation Layers: tensor([ 0,  5,  7, 11]), Selected3 Activation Layers: tensor([2, 4, 8, 9]),Selected1 Softmax Layers: tensor([ 2,  3,  8, 10]), Selected2 Softmax Layers: tensor([1, 4, 6, 9]), Selected3 Softmax Layers: tensor([ 0,  5,  7, 11]), Loss: 0.3793961993232369
